% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/costfunctiongrad.R
\name{costfunctiongrad}
\alias{costfunctiongrad}
\title{Gradient of cost function}
\usage{
costfunctiongrad(x, y, layers, hidden.units, lambda, th.vec, ...)
}
\arguments{
\item{x}{Matrix containing feature data to be supplied to the neural network for training.}

\item{y}{Vector containing label data corresponding to the rows of \code{x}.}

\item{layers}{Number of hidden layers in the neural network.}

\item{hidden.units}{Number of hidden units/neurons in each hidden layer.}

\item{lambda}{The regularization parameter. Must be a real number.}

\item{th.vec}{Vector containing initial weights.}

\item{...}{Further arguments passed to or from other methods.}
}
\value{
Vector containing partial derivatives of the cost function w.r.t each weight parameter.
}
\description{
Gradient of cost function evaluated with respect to each weight.
}
\details{
Calls the \code{forwardpass} function and isolates the \code{D} to be used during optimization.
}
\examples{
data <- data.matrix(iris)
x <- data[,1:4]
y <- data[,5]
w <- c(-28:27)/100
J <- costfunctiongrad(x,y,layers = 2,hidden.units = 3,lambda = 0.85,th.vec = w)
J1 <- costfunctiongrad(x,y,2,3,0.54,w)
}
